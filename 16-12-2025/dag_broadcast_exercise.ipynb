{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi7p05812taF",
        "outputId": "378c5d2d-ea42-462b-86bd-e597e7eeab4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 1 — RIDES (Large Fact Table)"
      ],
      "metadata": {
        "id": "ZVjjh8ySnLbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_data = [\n",
        "(\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "(\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "(\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "(\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "(\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "(\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "(\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "(\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "(\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "(\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]"
      ],
      "metadata": {
        "id": "Pp-gJwtBnCUv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rides_cols = [\n",
        "\"ride_id\",\n",
        "\"user_id\",\n",
        "\"city\",\n",
        "\"distance_km\",\n",
        "\"duration_seconds\",\n",
        "\"status\"\n",
        "]"
      ],
      "metadata": {
        "id": "RgnRbtEgnH8H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Rides Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)"
      ],
      "metadata": {
        "id": "ftVT2T1fnOqF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 2 — CITY SURGE MULTIPLIERS (Small Lookup)"
      ],
      "metadata": {
        "id": "_Ewpj2pYnhxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surge_data = [\n",
        "(\"Hyderabad\",1.2),\n",
        "(\"Delhi\",1.5),\n",
        "(\"Mumbai\",1.8),\n",
        "(\"Bangalore\",1.3)\n",
        "]"
      ],
      "metadata": {
        "id": "2Pz4hRNdnkHq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surge_cols = [\"city\",\"surge_multiplier\"]"
      ],
      "metadata": {
        "id": "k2P55VlInm1p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surge_df = spark.createDataFrame(surge_data, surge_cols)"
      ],
      "metadata": {
        "id": "Ky4bRg_Anoxh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a transformation pipeline"
      ],
      "metadata": {
        "id": "a5P0zm98nsDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5SaLas2Gn9v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No action triggered: Spark uses lazy evaluation. That means it doesn’t actually run the query or touch the data until an action is called.\n"
      ],
      "metadata": {
        "id": "IyQblwCMn7BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "transformed_rides = (\n",
        "    rides_df\n",
        "    .filter(col(\"status\") == \"Completed\")\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "d_L49iS_ns3L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trigger a single action on the pipeline."
      ],
      "metadata": {
        "id": "_x-UocB_oEJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_rides = (\n",
        "    rides_df\n",
        "    .filter(col(\"status\") == \"Completed\")\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")\n",
        "\n",
        "transformed_rides.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtl9XXqloGs9",
        "outputId": "dce192b0-3873-4e5a-f619-89e4cbcbbf6f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+\n",
            "|ride_id|     city|distance_km|\n",
            "+-------+---------+-----------+\n",
            "|   R001|Hyderabad|       12.5|\n",
            "|   R002|    Delhi|        8.2|\n",
            "|   R004|Bangalore|        5.5|\n",
            "|   R005|Hyderabad|       20.0|\n",
            "|   R006|    Delhi|       25.0|\n",
            "+-------+---------+-----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a transformation chain and run explain."
      ],
      "metadata": {
        "id": "COTYJtyApEoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "pipeline_df = (\n",
        "    rides_df\n",
        "    .filter(col(\"status\") == \"Completed\")\n",
        "    .filter(col(\"distance_km\") > 10)\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")\n",
        "\n",
        "pipeline_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwoTx_wZrQwH",
        "outputId": "58d73e83-9204-418c-d977-edb00eaa12d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double\n",
            "Project [ride_id#0, city#2, distance_km#3]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#0, city#2, distance_km#3]\n",
            "+- Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [ride_id#0, city#2, distance_km#3]\n",
            "+- *(1) Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- *(1) Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reorder transformations (filter after join vs before join)"
      ],
      "metadata": {
        "id": "kxnU8SGMrl7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = (\n",
        "    rides_df.join(surge_df, \"city\")\n",
        "            .filter(col(\"status\") == \"Completed\")\n",
        "            .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")"
      ],
      "metadata": {
        "id": "G_sO4zioroMW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_rised = rides_df.filter(col(\"status\") == \"Completed\")\n",
        "\n",
        "result_df = (\n",
        "    filtered_rised.join(surge_df, \"city\")\n",
        "                  .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")"
      ],
      "metadata": {
        "id": "0S6nn7Z3sfUt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PARTITIONS & SHUFFLE"
      ],
      "metadata": {
        "id": "YLrBATDCuYg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Partitions:\", rides_df.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX81J5_KuZVH",
        "outputId": "46762702-6846-4a62-e7af-3c7b3e293fc0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repartitions"
      ],
      "metadata": {
        "id": "5KEO-i84ul1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_repart = rides_df.repartition(4)\n",
        "\n",
        "print(\"Partitions after repartition:\", rides_repart.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FekZb9hpuoLW",
        "outputId": "3932fe28-c103-4053-d9c5-485b6265d11e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions after repartition: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coalesce"
      ],
      "metadata": {
        "id": "_F8msjzRuvAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rides_coalesce = rides_repart.coalesce(1)\n",
        "\n",
        "print(\"Partitions after coalesce:\", rides_coalesce.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0LCokIoutRx",
        "outputId": "0741a121-ab86-45c2-bcc2-a48f205897b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions after coalesce: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rides_repart.write.mode(\"overwrite\").parquet(\"output_repart\")\n",
        "rides_coalesce.write.mode(\"overwrite\").parquet(\"output_coalesce\")"
      ],
      "metadata": {
        "id": "CmsF-UO5wTU3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Repartition rides by city"
      ],
      "metadata": {
        "id": "eJ7wkVQ5whzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "rides_by_city = rides_df.repartition(\"city\")\n",
        "\n",
        "rides_by_city.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aKwA48nwlSS",
        "outputId": "67170a2e-e517-43ab-844d-6db6e866d6f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, user_id: string, city: string, distance_km: double, duration_seconds: bigint, status: string\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange hashpartitioning(city#2, 200), REPARTITION_BY_COL, [plan_id=160]\n",
            "   +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JOIN WITHOUT BROADCAST (BAD DAG)"
      ],
      "metadata": {
        "id": "xsdQ9w7MwwMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "joined_df = rides_df.join(surge_df, \"city\")\n",
        "\n",
        "joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvKmV9SDwyY5",
        "outputId": "96cb0e10-5c3e-4028-d438-5081bf8c35bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=187]\n",
            "      :     +- Filter isnotnull(city#2)\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=188]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply a filter ( distance_km > 10 ) before the join."
      ],
      "metadata": {
        "id": "qezYg0rZxNKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "filtered_rides = rides_df.filter(col(\"distance_km\") > 10)\n",
        "\n",
        "joined_df = filtered_rides.join(surge_df, \"city\")\n",
        "\n",
        "joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTGClL9AxPDU",
        "outputId": "1a5d34cb-c35a-4c84-8dc3-3c8cfe22d64c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#3 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (distance_km#3 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=218]\n",
            "      :     +- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=219]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BROADCAST JOIN (GOOD DAG)"
      ],
      "metadata": {
        "id": "QrgWuMdJxX7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply a broadcast hint to surge_df ."
      ],
      "metadata": {
        "id": "sJOn0nv4xY1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "joined_df = rides_df.join(broadcast(surge_df), \"city\")\n",
        "\n",
        "joined_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i36buQKxbv1",
        "outputId": "ec3918b2-1be8-4a73-aeb8-b2f8dcb48aab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6), rightHint=(strategy=broadcast)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- BroadcastHashJoin [city#2], [city#6], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(city#2)\n",
            "      :  +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=248]\n",
            "         +- Filter isnotnull(city#6)\n",
            "            +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# From the physical plan, identify all expensive operators and classify them as CPU, memory, or network heavy.\n",
        "\n",
        "- Join operations → CPU heavy, because they require comparing keys and combining rows.\n",
        "- Shuffle exchanges → Network heavy, because data must be moved across machines to align by keys.\n",
        "- Sort operations → CPU and memory heavy, since they require ordering large datasets and holding them in memory buffers.\n",
        "- Aggregation → CPU heavy, with memory overhead if intermediate results are large.\n",
        "\n",
        "\n",
        "# Why does Spark default to SortMergeJoin?\n",
        "\n",
        "Spark defaults to SortMergeJoin because it is the most scalable join strategy for large datasets. It sorts both sides of the join on the key and then merges them efficiently. This avoids broadcasting huge tables and handles cases where both inputs are large, making it a safe default for distributed systems.\n",
        "\n",
        "# Create a long transformation pipeline without any action. Explain what Spark has done so far.\n",
        "\n",
        "Spark has only recorded the sequence of steps in a logical plan. It has not processed any data yet. The system builds a blueprint of operations but delays execution until a result is explicitly requested.\n",
        "\n",
        "# Trigger different actions (count, show, write) separately. Observe whether Spark recomputes the DAG and explain behavior.\n",
        "\n",
        "Each action forces Spark to execute the entire pipeline. Unless results are cached, Spark recomputes the DAG from the beginning for every action. This happens because transformations are lazy, and Spark ensures correctness by re-running the plan whenever an action is invoked.\n",
        "\n",
        "# Why does broadcast remove shuffle from the DAG?\n",
        "\n",
        "Broadcast removes shuffle because the smaller dataset is copied to all machines. This means the larger dataset can be processed locally without redistributing rows across the network.\n",
        "\n",
        "# Why does repartition always introduce shuffle?\n",
        "\n",
        "Repartition introduces shuffle because data must be redistributed to create new partition boundaries. Rows are moved across machines to balance or align with the new partitioning scheme.\n",
        "\n",
        "# Why is coalesce cheaper than repartition?\n",
        "\n",
        "Coalesce is cheaper because it only reduces the number of partitions by merging existing ones. It avoids a full redistribution of data, so there is no network shuffle unless explicitly requested.\n",
        "\n",
        "# Why does Spark delay execution until an action?\n",
        "\n",
        "Spark delays execution to optimize performance. By waiting, it can combine multiple steps, eliminate redundancies, and minimize data movement. This lazy evaluation ensures resources are used efficiently and only when results are actually needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "v6xpAgpnyrt-"
      }
    }
  ]
}