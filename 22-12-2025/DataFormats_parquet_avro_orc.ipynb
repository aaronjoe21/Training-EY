{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ORD001\",\"Delhi\",\"Laptop\",45000,\"2024-01-05\"),\n",
        "    (\"ORD002\",\"Mumbai\",\"Mobile\",32000,\"2024-01-06\"),\n",
        "    (\"ORD003\",\"Bangalore\",\"Tablet\",30000,\"2024-01-07\"),\n",
        "    (\"ORD004\",\"Delhi\",\"Laptop\",55000,\"2024-01-08\"),\n",
        "    (\"ORD005\",\"Mumbai\",\"Tablet\",34000,\"2024-01-09\")\n",
        "]"
      ],
      "metadata": {
        "id": "387exhkIQ_F_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "MFTvy2pzRCNP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parquet is a data format used when\n",
        "##1. Dataset is large\n",
        "##2. Column wise compression is required during query processing"
      ],
      "metadata": {
        "id": "Ivq8qJS2RMod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"order_id\", \"city\", \"product\", \"price\", 'order_data']\n",
        "\n",
        "df=spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F13w6RO8RdQN",
        "outputId": "7f5c64c7-42b2-446d-e73c-aeb9fbae1f02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+----------+\n",
            "|order_id|     city|product|price|order_data|\n",
            "+--------+---------+-------+-----+----------+\n",
            "|  ORD001|    Delhi| Laptop|45000|2024-01-05|\n",
            "|  ORD002|   Mumbai| Mobile|32000|2024-01-06|\n",
            "|  ORD003|Bangalore| Tablet|30000|2024-01-07|\n",
            "|  ORD004|    Delhi| Laptop|55000|2024-01-08|\n",
            "|  ORD005|   Mumbai| Tablet|34000|2024-01-09|\n",
            "+--------+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"data/parquet/orders\")"
      ],
      "metadata": {
        "id": "vYEEHlrsR0xI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet=spark.read.parquet(\"data/parquet/orders\")\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlaoKcbR8Lv",
        "outputId": "5107c5c9-8673-44f2-f4d5-ad8fa9cc34cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+----------+\n",
            "|order_id|     city|product|price|order_data|\n",
            "+--------+---------+-------+-----+----------+\n",
            "|  ORD003|Bangalore| Tablet|30000|2024-01-07|\n",
            "|  ORD004|    Delhi| Laptop|55000|2024-01-08|\n",
            "|  ORD005|   Mumbai| Tablet|34000|2024-01-09|\n",
            "|  ORD001|    Delhi| Laptop|45000|2024-01-05|\n",
            "|  ORD002|   Mumbai| Mobile|32000|2024-01-06|\n",
            "+--------+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ORC\n",
        "## Used: When data requires very high compression\n",
        "## Example: Telecom/Insurance/Banking, etc historical data that is in petabytes"
      ],
      "metadata": {
        "id": "58C_FM0STT5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").orc(\"data/orc/orders\")"
      ],
      "metadata": {
        "id": "lJHi5UirTVMp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orc=spark.read.orc(\"data/orc/orders\")\n",
        "df_orc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEtNWE6AT7W1",
        "outputId": "c16e39e3-c715-4e64-fba4-5d80261c5279"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+----------+\n",
            "|order_id|     city|product|price|order_data|\n",
            "+--------+---------+-------+-----+----------+\n",
            "|  ORD003|Bangalore| Tablet|30000|2024-01-07|\n",
            "|  ORD004|    Delhi| Laptop|55000|2024-01-08|\n",
            "|  ORD005|   Mumbai| Tablet|34000|2024-01-09|\n",
            "|  ORD001|    Delhi| Laptop|45000|2024-01-05|\n",
            "|  ORD002|   Mumbai| Mobile|32000|2024-01-06|\n",
            "+--------+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AVRO"
      ],
      "metadata": {
        "id": "BilOLfrBaV4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pyspark\n",
        "!pip install pyspark==3.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "529_VQGaaX6a",
        "outputId": "17e7e96d-3358-40ef-84d3-87929b81b8d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 4.0.1\n",
            "Uninstalling pyspark-4.0.1:\n",
            "  Successfully uninstalled pyspark-4.0.1\n",
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.5.1)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=f8b2dc57e84e7d774abcfb968a06210b6c05dea9207781dfb1b913fb7abcffc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/91/5f/283b53010a8016a4ff1c4a1edd99bbe73afacb099645b5471b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.9\n",
            "    Uninstalling py4j-0.10.9.9:\n",
            "      Successfully uninstalled py4j-0.10.9.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              },
              "id": "4c9843118be345c49f28d8e67b03c705"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ORD001\",\"Delhi\",\"Laptop\",45000,\"2024-01-05\"),\n",
        "    (\"ORD002\",\"Mumbai\",\"Mobile\",32000,\"2024-01-06\"),\n",
        "    (\"ORD003\",\"Bangalore\",\"Tablet\",30000,\"2024-01-07\"),\n",
        "    (\"ORD004\",\"Delhi\",\"Laptop\",55000,\"2024-01-08\"),\n",
        "    (\"ORD005\",\"Mumbai\",\"Tablet\",34000,\"2024-01-09\")\n",
        "]\n",
        "\n",
        "columns=[\"order_id\", \"city\", \"product\", \"price\", 'order_data']"
      ],
      "metadata": {
        "id": "Ih-v4PKpa2da"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder\\\n",
        ".appName(\"AvroStable\")\\\n",
        ".config(\"spark.jars.packages\",\n",
        "        \"org.apache.spark:spark-avro_2.12:3.5.1\")\\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "hhHUwQBca5ni"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "U_95Ak4HbCQK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrlA3H1xbIrL",
        "outputId": "7a7ea74e-56fd-4836-b975-3c0a56f10b48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+----------+\n",
            "|order_id|     city|product|price|order_data|\n",
            "+--------+---------+-------+-----+----------+\n",
            "|  ORD001|    Delhi| Laptop|45000|2024-01-05|\n",
            "|  ORD002|   Mumbai| Mobile|32000|2024-01-06|\n",
            "|  ORD003|Bangalore| Tablet|30000|2024-01-07|\n",
            "|  ORD004|    Delhi| Laptop|55000|2024-01-08|\n",
            "|  ORD005|   Mumbai| Tablet|34000|2024-01-09|\n",
            "+--------+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"avro\").mode(\"overwrite\").save(\"/content/avro_out\")"
      ],
      "metadata": {
        "id": "_LKfDCyIbMMy"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}