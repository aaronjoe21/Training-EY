{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi7p05812taF",
        "outputId": "2cda12cc-4f58-40ae-8516-0f639cd7058d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Explicit Schema\n",
        "\n"
      ],
      "metadata": {
        "id": "MmvbAWAs1l03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n"
      ],
      "metadata": {
        "id": "FcegC_qz2NuX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"salary\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "uYMeFJSe2TF4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the data using the schema"
      ],
      "metadata": {
        "id": "Cuxd-eHY2T6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "jfUaFV782ymL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_users = [\n",
        "    (\"U001\",\"Amit\",\"29\",\"Hyderabad\",\"50000\"),\n",
        "    (\"U002\",\"Neha\",\"Thirty Two\",\"Delhi\",\"62000\"),\n",
        "    (\"U003\",\"Ravi\",None,\"Bangalore\",\"45k\"),\n",
        "    (\"U004\",\"Pooja\",\"28\",\"Mumbai\",58000),\n",
        "    (\"U005\",None,\"31\",\"Chennai\",\"\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(raw_users, schema=user_schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etkJB47-2l6F",
        "outputId": "bd060fa7-2f9c-49a1-dcfe-0ef4973f515d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----------+---------+------+\n",
            "|user_id| name|       age|     city|salary|\n",
            "+-------+-----+----------+---------+------+\n",
            "|   U001| Amit|        29|Hyderabad| 50000|\n",
            "|   U002| Neha|Thirty Two|    Delhi| 62000|\n",
            "|   U003| Ravi|      NULL|Bangalore|   45k|\n",
            "|   U004|Pooja|        28|   Mumbai| 58000|\n",
            "|   U005| NULL|        31|  Chennai|      |\n",
            "+-------+-----+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify records that fail type conversion"
      ],
      "metadata": {
        "id": "1aW_xF2p27Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_cast = df.withColumn(\"age_int\", col(\"age\").cast(\"int\")) \\\n",
        "            .withColumn(\"salary_str\", col(\"salary\").cast(\"string\"))\n",
        "\n",
        "df_cast.show()\n",
        "\n",
        "#Shows Error"
      ],
      "metadata": {
        "id": "w-0iabOW3KwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Convert Age to Integer Safely"
      ],
      "metadata": {
        "id": "1SyxQc673L1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, regexp_extract\n",
        "\n",
        "df_clean_age = df.withColumn(\n",
        "    \"age_int\",\n",
        "    when(col(\"age\").rlike(\"^[0-9]+$\"), col(\"age\").cast(\"int\"))\n",
        ")"
      ],
      "metadata": {
        "id": "m5FxGwo43b3_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize salary into integer (handle k )"
      ],
      "metadata": {
        "id": "2nW20t3w3eSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "df_clean_salary = df_clean_age.withColumn(\n",
        "    \"salary_int\",\n",
        "    when(col(\"salary\").rlike(\"^[0-9]+$\"), col(\"salary\").cast(\"int\"))\n",
        "    .when(col(\"salary\").rlike(\"^[0-9]+k$\"),\n",
        "          (regexp_extract(col(\"salary\"), \"([0-9]+)\", 1).cast(\"int\") * 1000))\n",
        "    .otherwise(None)\n",
        ")"
      ],
      "metadata": {
        "id": "Lzdj2s1w3gd9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace Missing Names With \"UNKNOWN\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TgJVAcqr3kD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fix_names = df_clean_salary.withColumn(\n",
        "    \"name_fixed\",\n",
        "    when(col(\"name\").isNull(), \"UNKNOWN\").otherwise(col(\"name\"))\n",
        ")"
      ],
      "metadata": {
        "id": "54_n5yIo3o41"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop Records Where Age Cannot Be Recovered"
      ],
      "metadata": {
        "id": "xOhsxDPe3sXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_fix_names.filter(col(\"age_int\").isNotNull())"
      ],
      "metadata": {
        "id": "8CgOcAM03tSS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Clean DataFrame"
      ],
      "metadata": {
        "id": "AwkAie4A3-h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.select(\"user_id\",\"name_fixed\",\"age_int\",\"city\",\"salary_int\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROvWVP9D4BTh",
        "outputId": "0b1f9abe-24f7-4b87-9253-c21ecb915f0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+---------+----------+\n",
            "|user_id|name_fixed|age_int|     city|salary_int|\n",
            "+-------+----------+-------+---------+----------+\n",
            "|   U001|      Amit|     29|Hyderabad|     50000|\n",
            "|   U004|     Pooja|     28|   Mumbai|     58000|\n",
            "|   U005|   UNKNOWN|     31|  Chennai|      NULL|\n",
            "+-------+----------+-------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  E-COMMERCE ORDERS (ARRAY CORRUPTION)"
      ],
      "metadata": {
        "id": "CpteeDa04E5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Schema"
      ],
      "metadata": {
        "id": "eHk_R_DI4chD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "order_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"items\", ArrayType(StringType()), True),   # enforce array\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "EpEAUrXw4blu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Raw Data with Schema"
      ],
      "metadata": {
        "id": "TD_IR7yF4ht7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "    (\"O001\",\"U001\",[\"Laptop\",\"Mobile\",\"Tablet\"],75000),\n",
        "    (\"O002\",\"U002\",[\"Mobile\",\"Tablet\"],32000),\n",
        "    (\"O003\",\"U003\",[\"Laptop\"],72000),\n",
        "    (\"O004\",\"U004\",[],25000),   # null â†’ empty array\n",
        "    (\"O005\",\"U005\",[\"Laptop\",\"Mobile\"],68000)\n",
        "]\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "order_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"items\", ArrayType(StringType()), True),\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(raw_orders, schema=order_schema)"
      ],
      "metadata": {
        "id": "ku_LrbeI4jsT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize All Item Values into Arrays"
      ],
      "metadata": {
        "id": "dINgoWqQ5qjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "    (\"O001\",\"U001\",\"Laptop,Mobile,Tablet\",75000),\n",
        "    (\"O002\",\"U002\",\"Mobile,Tablet\",32000),\n",
        "    (\"O003\",\"U003\",\"Laptop\",72000),\n",
        "    (\"O004\",\"U004\",None,25000),\n",
        "    (\"O005\",\"U005\",\"Laptop|Mobile\",68000)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(raw_orders, [\"order_id\",\"user_id\",\"items\",\"amount\"])\n",
        "\n",
        "from pyspark.sql.functions import when, split, col, regexp_replace, expr\n",
        "\n",
        "df_norm = df.withColumn(\n",
        "    \"items_array\",\n",
        "    when(col(\"items\").isNull(), expr(\"array()\"))\n",
        "    .otherwise(\n",
        "        split(regexp_replace(col(\"items\"), \"\\\\|\", \",\"), \",\")\n",
        "    )\n",
        ")\n",
        "df_norm.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RZSokJG69E7",
        "outputId": "712430b8-0bdf-4632-9103-1e616ea13ef1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------------------+------+------------------------+\n",
            "|order_id|user_id|items               |amount|items_array             |\n",
            "+--------+-------+--------------------+------+------------------------+\n",
            "|O001    |U001   |Laptop,Mobile,Tablet|75000 |[Laptop, Mobile, Tablet]|\n",
            "|O002    |U002   |Mobile,Tablet       |32000 |[Mobile, Tablet]        |\n",
            "|O003    |U003   |Laptop              |72000 |[Laptop]                |\n",
            "|O004    |U004   |NULL                |25000 |[]                      |\n",
            "|O005    |U005   |Laptop|Mobile       |68000 |[Laptop, Mobile]        |\n",
            "+--------+-------+--------------------+------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explode Items into One Row per Item"
      ],
      "metadata": {
        "id": "tn60PldQ7SYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_exploded = df_norm.withColumn(\"item\", explode(col(\"items_array\")))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0bePQsN7Z8R",
        "outputId": "62ab9727-c9db-4893-a034-77723c68e171"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------------------+------+--------------------+------+\n",
            "|order_id|user_id|               items|amount|         items_array|  item|\n",
            "+--------+-------+--------------------+------+--------------------+------+\n",
            "|    O001|   U001|Laptop,Mobile,Tablet| 75000|[Laptop, Mobile, ...|Laptop|\n",
            "|    O001|   U001|Laptop,Mobile,Tablet| 75000|[Laptop, Mobile, ...|Mobile|\n",
            "|    O001|   U001|Laptop,Mobile,Tablet| 75000|[Laptop, Mobile, ...|Tablet|\n",
            "|    O002|   U002|       Mobile,Tablet| 32000|    [Mobile, Tablet]|Mobile|\n",
            "|    O002|   U002|       Mobile,Tablet| 32000|    [Mobile, Tablet]|Tablet|\n",
            "|    O003|   U003|              Laptop| 72000|            [Laptop]|Laptop|\n",
            "|    O005|   U005|       Laptop|Mobile| 68000|    [Laptop, Mobile]|Laptop|\n",
            "|    O005|   U005|       Laptop|Mobile| 68000|    [Laptop, Mobile]|Mobile|\n",
            "+--------+-------+--------------------+------+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Frequency of Each Item\n"
      ],
      "metadata": {
        "id": "d0zPcVkx9CK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_item_freq = df_exploded.groupBy(\"item\").count()\n",
        "df_item_freq.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJkLlc899GXZ",
        "outputId": "e3607fef-1237-4575-ad5f-0df6d7bc1e7b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  item|count|\n",
            "+------+-----+\n",
            "|Laptop|    3|\n",
            "|Mobile|    3|\n",
            "|Tablet|    2|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Orders with More Than 2 Items"
      ],
      "metadata": {
        "id": "2UciGRtP9JE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "df_multi_items = df_norm.filter(size(col(\"items_array\")) > 2)\n",
        "df_multi_items.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8GqKrPo9LVw",
        "outputId": "bce01e55-9417-48c7-9a7f-ffb6d2a12abb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------------------+------+--------------------+\n",
            "|order_id|user_id|               items|amount|         items_array|\n",
            "+--------+-------+--------------------+------+--------------------+\n",
            "|    O001|   U001|Laptop,Mobile,Tablet| 75000|[Laptop, Mobile, ...|\n",
            "+--------+-------+--------------------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "##  Define Schema"
      ],
      "metadata": {
        "id": "noW579nm9Tgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, MapType, IntegerType\n",
        "\n",
        "device_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"usage\", MapType(StringType(), IntegerType()), True)\n",
        "])"
      ],
      "metadata": {
        "id": "GwCa_L779cmk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Raw Data\n",
        "\n"
      ],
      "metadata": {
        "id": "7IZdBuhj9hQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_data = [\n",
        "    (\"U001\", {\"mobile\": 120, \"laptop\": 300}),\n",
        "    (\"U002\", {\"mobile\": 200, \"tablet\": 100}),\n",
        "    (\"U003\", {\"desktop\": 400, \"mobile\": 150}),\n",
        "    (\"U004\", {}),\n",
        "    (\"U005\", {\"laptop\": 250})\n",
        "]\n",
        "\n",
        "df_devices = spark.createDataFrame(device_data, device_schema)\n",
        "df_devices.printSchema()\n",
        "df_devices.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YCsIItiBUx_",
        "outputId": "e619bbe7-3323-4068-dc52-a2abd9c9e85b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n",
            "+-------+-------------------------------+\n",
            "|user_id|usage                          |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |\n",
            "|U003   |{mobile -> 150, desktop -> 400}|\n",
            "|U004   |{}                             |\n",
            "|U005   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract mobile usage safely"
      ],
      "metadata": {
        "id": "i92UicVqFnqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_mobile = df_devices.withColumn(\"mobile_usage\", col(\"usage\")[\"mobile\"])\n",
        "df_mobile.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ved2dGDBFoxP",
        "outputId": "2db62c00-5881-4bd1-b3e3-73339d466615"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------------+------------+\n",
            "|user_id|usage                          |mobile_usage|\n",
            "+-------+-------------------------------+------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |120         |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |200         |\n",
            "|U003   |{mobile -> 150, desktop -> 400}|150         |\n",
            "|U004   |{}                             |NULL        |\n",
            "|U005   |{laptop -> 250}                |NULL        |\n",
            "+-------+-------------------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify users with usage above a threshold"
      ],
      "metadata": {
        "id": "0dYbsMYAGOQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_threshold = df_mobile.filter(col(\"mobile_usage\") > 150)\n",
        "df_threshold.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVILjyUWGRFp",
        "outputId": "926ab04a-b8dc-477d-bc50-45b8def3f82a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------+------------+\n",
            "|user_id|usage                         |mobile_usage|\n",
            "+-------+------------------------------+------------+\n",
            "|U002   |{mobile -> 200, tablet -> 100}|200         |\n",
            "+-------+------------------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 4"
      ],
      "metadata": {
        "id": "e2HdhC0IGTXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "address_schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"state\", StringType(), True),\n",
        "    StructField(\"pincode\", StringType(), True)\n",
        "])\n",
        "\n",
        "profile_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"address\", address_schema, True)\n",
        "])"
      ],
      "metadata": {
        "id": "Whx34gX7GgN2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Raw Data\n"
      ],
      "metadata": {
        "id": "GA8f1HUrHH6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_profiles = [\n",
        "    (\"U001\",\"Hyderabad,Telangana,500081\"),\n",
        "    (\"U002\",{\"city\":\"Delhi\",\"state\":\"Delhi\",\"pincode\":\"110001\"}),\n",
        "    (\"U003\",(\"Bangalore\",\"Karnataka\",560001)),\n",
        "    (\"U004\",\"Mumbai,MH\"),\n",
        "    (\"U005\",None)\n",
        "]\n",
        "\n",
        "\n",
        "normalized_profiles = []\n",
        "for user_id, addr in raw_profiles:\n",
        "    if addr is None:\n",
        "        normalized_profiles.append((user_id, {\"city\": None, \"state\": None, \"pincode\": None}))\n",
        "    elif isinstance(addr, str):\n",
        "        parts = addr.split(\",\")\n",
        "        city = parts[0] if len(parts) > 0 else None\n",
        "        state = parts[1] if len(parts) > 1 else None\n",
        "        pincode = parts[2] if len(parts) > 2 else None\n",
        "        normalized_profiles.append((user_id, {\"city\": city, \"state\": state, \"pincode\": str(pincode) if pincode else None}))\n",
        "    elif isinstance(addr, dict):\n",
        "        normalized_profiles.append((user_id, {\"city\": addr.get(\"city\"), \"state\": addr.get(\"state\"), \"pincode\": str(addr.get(\"pincode\"))}))\n",
        "    elif isinstance(addr, tuple):\n",
        "        city = addr[0] if len(addr) > 0 else None\n",
        "        state = addr[1] if len(addr) > 1 else None\n",
        "        pincode = addr[2] if len(addr) > 2 else None\n",
        "        normalized_profiles.append((user_id, {\"city\": city, \"state\": state, \"pincode\": str(pincode) if pincode else None}))\n",
        "    else:\n",
        "        normalized_profiles.append((user_id, {\"city\": None, \"state\": None, \"pincode\": None}))\n"
      ],
      "metadata": {
        "id": "cTVjpoSALEpo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_profiles = spark.createDataFrame(normalized_profiles, profile_schema)\n",
        "df_profiles.printSchema()\n",
        "df_profiles.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7XND5oHMSX",
        "outputId": "6321565e-0175-4387-cab0-baa7ab4bf7bd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- pincode: string (nullable = true)\n",
            "\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "|U004   |{Mumbai, MH, NULL}            |\n",
            "|U005   |{NULL, NULL, NULL}            |\n",
            "+-------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract City, State, Pincode Safely"
      ],
      "metadata": {
        "id": "oLc5IC_BLwDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_extracted = df_profiles.select(\n",
        "    \"user_id\",\n",
        "    col(\"address.city\").alias(\"city\"),\n",
        "    col(\"address.state\").alias(\"state\"),\n",
        "    col(\"address.pincode\").alias(\"pincode\")\n",
        ")\n",
        "df_extracted.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuhuWXWzHHbA",
        "outputId": "f4f0246e-d425-4490-98fb-6c4932284f1c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-------+\n",
            "|user_id|city     |state    |pincode|\n",
            "+-------+---------+---------+-------+\n",
            "|U001   |Hyderabad|Telangana|500081 |\n",
            "|U002   |Delhi    |Delhi    |110001 |\n",
            "|U003   |Bangalore|Karnataka|560001 |\n",
            "|U004   |Mumbai   |MH       |NULL   |\n",
            "|U005   |NULL     |NULL     |NULL   |\n",
            "+-------+---------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Default Pincode When Missing"
      ],
      "metadata": {
        "id": "oSUycPf7NIH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_fixed = df_extracted.withColumn(\n",
        "    \"pincode\",\n",
        "    when(col(\"pincode\").isNull(), \"000000\").otherwise(col(\"pincode\"))\n",
        ")\n",
        "df_fixed.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqQr7DYXNHbe",
        "outputId": "95e9c8f1-8acf-4b77-e8be-62dd380182e3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-------+\n",
            "|user_id|city     |state    |pincode|\n",
            "+-------+---------+---------+-------+\n",
            "|U001   |Hyderabad|Telangana|500081 |\n",
            "|U002   |Delhi    |Delhi    |110001 |\n",
            "|U003   |Bangalore|Karnataka|560001 |\n",
            "|U004   |Mumbai   |MH       |000000 |\n",
            "|U005   |NULL     |NULL     |000000 |\n",
            "+-------+---------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop Irrecoverable Records\n"
      ],
      "metadata": {
        "id": "1RcTxcKgNMsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_fixed.filter(col(\"city\").isNotNull() & col(\"state\").isNotNull())\n",
        "df_clean.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEw-iCQ2NRZW",
        "outputId": "2f1adce8-39f5-4d89-a551-b0665c1d2e86"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-------+\n",
            "|user_id|city     |state    |pincode|\n",
            "+-------+---------+---------+-------+\n",
            "|U001   |Hyderabad|Telangana|500081 |\n",
            "|U002   |Delhi    |Delhi    |110001 |\n",
            "|U003   |Bangalore|Karnataka|560001 |\n",
            "|U004   |Mumbai   |MH       |000000 |\n",
            "+-------+---------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data\n",
        "\n"
      ],
      "metadata": {
        "id": "gMC2Lb0FkoCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_transactions = [\n",
        "    (\"T001\",\"2024-01-05\",\"45000\"),\n",
        "    (\"T002\",\"05/01/2024\",52000),\n",
        "    (\"T003\",\"Jan 06 2024\",\"Thirty Thousand\"),\n",
        "    (\"T004\",None,38000),\n",
        "    (\"T005\",\"2024/01/07\",\"42000\")\n",
        "]"
      ],
      "metadata": {
        "id": "F9qHX2Jmks1p"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize All Rows to One Format"
      ],
      "metadata": {
        "id": "Au_VyEatkuqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "normalized_transactions = []\n",
        "for tid, date, amt in raw_transactions:\n",
        "    norm_date = None\n",
        "    if date:\n",
        "        for fmt in [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%b %d %Y\", \"%Y/%m/%d\"]:\n",
        "            try:\n",
        "                norm_date = datetime.strptime(date, fmt).strftime(\"%Y-%m-%d\")\n",
        "                break\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    norm_amt = None\n",
        "    try:\n",
        "        norm_amt = int(str(amt).replace(\",\", \"\"))\n",
        "    except Exception:\n",
        "        norm_amt = None\n",
        "\n",
        "    normalized_transactions.append((tid, norm_date, norm_amt))\n",
        "\n",
        "for row in normalized_transactions:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhf2bROqkxxC",
        "outputId": "de869c65-10ba-4e69-a866-7396497dd91f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T001', '2024-01-05', 45000)\n",
            "('T002', '2024-01-05', 52000)\n",
            "('T003', '2024-01-06', None)\n",
            "('T004', None, 38000)\n",
            "('T005', '2024-01-07', 42000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Schema"
      ],
      "metadata": {
        "id": "CQ5d1wpLk4nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType\n",
        "\n",
        "transaction_schema = StructType([\n",
        "    StructField(\"trans_id\", StringType(), False),\n",
        "    StructField(\"trans_date\", StringType(), True),\n",
        "    StructField(\"amount\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "eIjTeGF3k9ml"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "NSZR4-zRlAmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transactions = spark.createDataFrame(normalized_transactions, transaction_schema)\n",
        "df_transactions.printSchema()\n",
        "df_transactions.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHyuAVQ3lE9E",
        "outputId": "b8a34d7e-7c38-4820-bf8b-f3d96fceb255"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- trans_id: string (nullable = false)\n",
            " |-- trans_date: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            "\n",
            "+--------+----------+------+\n",
            "|trans_id|trans_date|amount|\n",
            "+--------+----------+------+\n",
            "|T001    |2024-01-05|45000 |\n",
            "|T002    |2024-01-05|52000 |\n",
            "|T003    |2024-01-06|NULL  |\n",
            "|T004    |NULL      |38000 |\n",
            "|T005    |2024-01-07|42000 |\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Unrecoverable Records"
      ],
      "metadata": {
        "id": "aIaEqgBql2uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_invalid = df_transactions.filter(col(\"trans_date\").isNull() | col(\"amount\").isNull())\n",
        "df_invalid.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk3gizN3l3jt",
        "outputId": "79016c76-c758-416f-b5fa-5bcf41ba868b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|trans_id|trans_date|amount|\n",
            "+--------+----------+------+\n",
            "|T003    |2024-01-06|NULL  |\n",
            "|T004    |NULL      |38000 |\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate Valid vs Invalid Transactions"
      ],
      "metadata": {
        "id": "EOLEv_MYl8jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = df_transactions.filter(col(\"trans_date\").isNotNull() & col(\"amount\").isNotNull())\n",
        "df_valid.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5CmUzsIk38v",
        "outputId": "a762b6cc-e4e1-4521-a40f-4af8e2509503"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|trans_id|trans_date|amount|\n",
            "+--------+----------+------+\n",
            "|T001    |2024-01-05|45000 |\n",
            "|T002    |2024-01-05|52000 |\n",
            "|T005    |2024-01-07|42000 |\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}