{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi7p05812taF",
        "outputId": "9b5fc2fa-c1fd-4aeb-ef03-b2cb75b178e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "jfUaFV782ymL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design an explicit schema using StructType"
      ],
      "metadata": {
        "id": "rgECX_ROHB0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "user_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"skills\", ArrayType(StringType()), True)\n",
        "])"
      ],
      "metadata": {
        "id": "liv2P43CELv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize age into IntegerType"
      ],
      "metadata": {
        "id": "Wtv-yEq1HECe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_age(age):\n",
        "    if age is None:\n",
        "        return None\n",
        "    if isinstance(age, int):\n",
        "        return age\n",
        "    if isinstance(age, str):\n",
        "        age = age.strip()\n",
        "        if age.isdigit():\n",
        "            return int(age)\n",
        "        word_to_num = {\n",
        "            \"Thirty\": 30, \"Twenty\": 20, \"Twenty Eight\": 28,\n",
        "            \"Twenty Nine\": 29, \"Thirty One\": 31\n",
        "        }\n",
        "        return word_to_num.get(age, None)\n",
        "    return None"
      ],
      "metadata": {
        "id": "P4eLi-XdENRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def normalize_skills(skills):\n",
        "    if skills is None:\n",
        "        return []\n",
        "    if isinstance(skills, list):\n",
        "        return [s.strip() for s in skills]\n",
        "    if isinstance(skills, str):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(skills)\n",
        "            if isinstance(parsed, list):\n",
        "                return [s.strip() for s in parsed]\n",
        "        except:\n",
        "            return [s.strip() for s in skills.split(\",\")]\n",
        "    return []"
      ],
      "metadata": {
        "id": "pjLqdvJgET5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_name(name):\n",
        "    if name is None or name.strip() == \"\":\n",
        "        return \"Unknown\"\n",
        "    return name.strip()"
      ],
      "metadata": {
        "id": "3HQPtu2EEWuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_users = [\n",
        "    (\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "    (\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "    (\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "    (\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "    (\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n",
        "]\n",
        "\n",
        "clean_data = []\n",
        "for uid, name, age, city, skills in raw_users:\n",
        "    clean_data.append((\n",
        "        uid,\n",
        "        normalize_name(name),\n",
        "        normalize_age(age),\n",
        "        city,\n",
        "        normalize_skills(skills)\n",
        "    ))\n",
        "\n",
        "users_df = spark.createDataFrame(clean_data, schema=user_schema)\n",
        "users_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jILJVSOJEYCD",
        "outputId": "027a6127-6fcc-4dd8-9531-aca2481d4d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|name   |age |city     |skills         |\n",
            "+-------+-------+----+---------+---------------+\n",
            "|U001   |Amit   |28  |Hyderabad|[AI, ML, Cloud]|\n",
            "|U002   |Neha   |30  |Delhi    |[AI, Testing]  |\n",
            "|U003   |Ravi   |NULL|Bangalore|[Data, Spark]  |\n",
            "|U004   |Pooja  |29  |Mumbai   |[]             |\n",
            "|U005   |Unknown|31  |Chennai  |[DevOps]       |\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "XeuYO19nKjD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_courses = [\n",
        " (\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        " (\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        " (\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        " (\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "MpyWsfoVKkWd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Nested StructType\n",
        "\n"
      ],
      "metadata": {
        "id": "3vA8O_zYLA5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"metadata\", StructType([\n",
        "        StructField(\"domain\", StringType(), True),\n",
        "        StructField(\"level\", StringType(), True)\n",
        "    ])),\n",
        "    StructField(\"price\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "1fqlbKMvLGRt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lAuVFKtCLJ2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_courses = [\n",
        " (\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        " (\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        " (\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        " (\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]\n",
        "\n",
        "def normalize_metadata(meta):\n",
        "    if isinstance(meta, tuple):\n",
        "        return {\"domain\": meta[0], \"level\": meta[1]}\n",
        "    elif isinstance(meta, dict):\n",
        "        return {\"domain\": meta.get(\"domain\"), \"level\": meta.get(\"level\")}\n",
        "    elif isinstance(meta, str):\n",
        "        parts = meta.split(\"|\")\n",
        "        return {\"domain\": parts[0], \"level\": parts[1]}\n",
        "    else:\n",
        "        return {\"domain\": None, \"level\": None}\n",
        "\n",
        "def normalize_price(price):\n",
        "    if price is None:\n",
        "        return 0\n",
        "    return int(str(price).replace(\"₹\",\"\"))\n",
        "\n",
        "fixed_courses = [\n",
        "    (cid, title, normalize_metadata(meta), normalize_price(price))\n",
        "    for cid, title, meta, price in raw_courses\n",
        "]"
      ],
      "metadata": {
        "id": "JHwmMubOLTJy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataFrame\n"
      ],
      "metadata": {
        "id": "z3t4yIpKLZ9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CoursesDF\").getOrCreate()\n",
        "\n",
        "courses_df = spark.createDataFrame(fixed_courses, schema=course_schema)\n",
        "courses_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YSD657MLi1R",
        "outputId": "8f08e8be-c7e4-462e-d5ae-f4e8f4794c68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------------------+-----+\n",
            "|course_id|title                    |metadata                    |price|\n",
            "+---------+-------------------------+----------------------------+-----+\n",
            "|C001     |PySpark Mastery          |{Data Engineering, Advanced}|9999 |\n",
            "|C002     |AI for Testers           |{QA, Beginner}              |8999 |\n",
            "|C003     |ML Foundations           |{AI, Intermediate}          |0    |\n",
            "|C004     |Data Engineering Bootcamp|{Data, Advanced}            |14999|\n",
            "+---------+-------------------------+----------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 3"
      ],
      "metadata": {
        "id": "Crsz4jBVMCmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_enrollments = [\n",
        " (\"U001\",\"C001\",\"2024-01-05\"),\n",
        " (\"U002\",\"C002\",\"05/01/2024\"),\n",
        " (\"U003\",\"C001\",\"2024/01/06\"),\n",
        " (\"U004\",\"C003\",\"invalid_date\"),\n",
        " (\"U001\",\"C004\",\"2024-01-10\")\n",
        "]"
      ],
      "metadata": {
        "id": "D3BV_HnqL3F-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Enrollment Dates\n"
      ],
      "metadata": {
        "id": "K5ucE-WOMR8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col, when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Enrollments\").getOrCreate()\n",
        "\n",
        "enrollments_df = spark.createDataFrame(raw_enrollments, [\"user_id\",\"course_id\",\"enroll_date_raw\"])\n",
        "\n",
        "enrollments_df = enrollments_df.withColumn(\n",
        "    \"enroll_date\",\n",
        "    when(to_date(col(\"enroll_date_raw\"), \"yyyy-MM-dd\").isNotNull(),\n",
        "         to_date(col(\"enroll_date_raw\"), \"yyyy-MM-dd\"))\n",
        "    .when(to_date(col(\"enroll_date_raw\"), \"dd/MM/yyyy\").isNotNull(),\n",
        "         to_date(col(\"enroll_date_raw\"), \"dd/MM/yyyy\"))\n",
        "    .when(to_date(col(\"enroll_date_raw\"), \"yyyy/MM/dd\").isNotNull(),\n",
        "         to_date(col(\"enroll_date_raw\"), \"yyyy/MM/dd\"))\n",
        ")"
      ],
      "metadata": {
        "id": "gtj2HKm_MXQG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Invalid Enrollments\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ANhJW9KqMaDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "raw_enrollments = [\n",
        " (\"U001\",\"C001\",\"2024-01-05\"),\n",
        " (\"U002\",\"C002\",\"05/01/2024\"),\n",
        " (\"U003\",\"C001\",\"2024/01/06\"),\n",
        " (\"U004\",\"C003\",\"invalid_date\"),\n",
        " (\"U001\",\"C004\",\"2024-01-10\")\n",
        "]\n",
        "\n",
        "enrollments_df = spark.createDataFrame(raw_enrollments, [\"user_id\",\"course_id\",\"enroll_date_raw\"])\n",
        "\n",
        "enrollments_df = enrollments_df.withColumn(\n",
        "    \"enroll_date\",\n",
        "    F.coalesce(\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'yyyy-MM-dd')\"),\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'dd/MM/yyyy')\"),\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'yyyy/MM/dd')\")\n",
        "    )\n",
        ").withColumn(\n",
        "    \"is_valid\",\n",
        "    F.col(\"enroll_date\").isNotNull()\n",
        ")\n",
        "\n",
        "enrollments_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSkiu1zqMh7C",
        "outputId": "99fe438a-d111-402a-99d4-aaba3c542e5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------------+-------------------+--------+\n",
            "|user_id|course_id|enroll_date_raw|enroll_date        |is_valid|\n",
            "+-------+---------+---------------+-------------------+--------+\n",
            "|U001   |C001     |2024-01-05     |2024-01-05 00:00:00|true    |\n",
            "|U002   |C002     |05/01/2024     |2024-01-05 00:00:00|true    |\n",
            "|U003   |C001     |2024/01/06     |2024-01-06 00:00:00|true    |\n",
            "|U004   |C003     |invalid_date   |NULL               |false   |\n",
            "|U001   |C004     |2024-01-10     |2024-01-10 00:00:00|true    |\n",
            "+-------+---------+---------------+-------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join"
      ],
      "metadata": {
        "id": "sP3gpaQZWzt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = spark.createDataFrame([\n",
        "    (\"U001\",\"Alice\"),\n",
        "    (\"U002\",\"Bob\"),\n",
        "    (\"U003\",\"Charlie\"),\n",
        "    (\"U004\",\"Diana\")\n",
        "], [\"user_id\",\"user_name\"])\n"
      ],
      "metadata": {
        "id": "3UpFkfYlXFXt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "raw_enrollments = [\n",
        " (\"U001\",\"C001\",\"2024-01-05\"),\n",
        " (\"U002\",\"C002\",\"05/01/2024\"),\n",
        " (\"U003\",\"C001\",\"2024/01/06\"),\n",
        " (\"U004\",\"C003\",\"invalid_date\"),\n",
        " (\"U001\",\"C004\",\"2024-01-10\")\n",
        "]\n",
        "\n",
        "enrollments_df = spark.createDataFrame(raw_enrollments, [\"user_id\",\"course_id\",\"enroll_date_raw\"])\n",
        "\n",
        "enrollments_df = enrollments_df.withColumn(\n",
        "    \"enroll_date\",\n",
        "    F.coalesce(\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'yyyy-MM-dd')\"),\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'dd/MM/yyyy')\"),\n",
        "        F.expr(\"try_to_timestamp(enroll_date_raw, 'yyyy/MM/dd')\")\n",
        "    )\n",
        ").withColumn(\"is_valid\", F.col(\"enroll_date\").isNotNull())"
      ],
      "metadata": {
        "id": "zGnbNw3NXAOH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "enrollments_users = enrollments_df.join(users_df, \"user_id\", \"left\")\n",
        "\n",
        "full_enrollments = enrollments_users.join(courses_df, \"course_id\", \"left\")\n",
        "\n",
        "full_enrollments = enrollments_df \\\n",
        "    .join(broadcast(users_df), \"user_id\", \"left\") \\\n",
        "    .join(broadcast(courses_df), \"course_id\", \"left\")"
      ],
      "metadata": {
        "id": "zowasrWKW1os"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_enrollments.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSPRvbm0XNJc",
        "outputId": "597b4d0f-6802-4936-f44b-f1dc556ebbcf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69]\n",
            ":  +- Join LeftOuter, (user_id#70 = user_id#68)\n",
            ":     :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, isnotnull(enroll_date#73) AS is_valid#74]\n",
            ":     :  +- Project [user_id#70, course_id#71, enroll_date_raw#72, coalesce(try_to_timestamp(enroll_date_raw#72, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(enroll_date_raw#72, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(enroll_date_raw#72, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS enroll_date#73]\n",
            ":     :     +- LogicalRDD [user_id#70, course_id#71, enroll_date_raw#72], false\n",
            ":     +- ResolvedHint (strategy=broadcast)\n",
            ":        +- LogicalRDD [user_id#68, user_name#69], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [course_id#0, title#1, metadata#2, price#3], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enroll_date_raw: string, enroll_date: timestamp, is_valid: boolean, user_name: string, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#71, user_id#70, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69, title#1, metadata#2, price#3]\n",
            "+- Join LeftOuter, (course_id#71 = course_id#0)\n",
            "   :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69]\n",
            "   :  +- Join LeftOuter, (user_id#70 = user_id#68)\n",
            "   :     :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, isnotnull(enroll_date#73) AS is_valid#74]\n",
            "   :     :  +- Project [user_id#70, course_id#71, enroll_date_raw#72, coalesce(try_to_timestamp(enroll_date_raw#72, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(enroll_date_raw#72, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(enroll_date_raw#72, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) AS enroll_date#73]\n",
            "   :     :     +- LogicalRDD [user_id#70, course_id#71, enroll_date_raw#72], false\n",
            "   :     +- ResolvedHint (strategy=broadcast)\n",
            "   :        +- LogicalRDD [user_id#68, user_name#69], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [course_id#0, title#1, metadata#2, price#3], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#71, user_id#70, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69, title#1, metadata#2, price#3]\n",
            "+- Join LeftOuter, (course_id#71 = course_id#0), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69]\n",
            "   :  +- Join LeftOuter, (user_id#70 = user_id#68), rightHint=(strategy=broadcast)\n",
            "   :     :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, isnotnull(enroll_date#73) AS is_valid#74]\n",
            "   :     :  +- Project [user_id#70, course_id#71, enroll_date_raw#72, coalesce(gettimestamp(enroll_date_raw#72, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(enroll_date_raw#72, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(enroll_date_raw#72, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS enroll_date#73]\n",
            "   :     :     +- LogicalRDD [user_id#70, course_id#71, enroll_date_raw#72], false\n",
            "   :     +- Filter isnotnull(user_id#68)\n",
            "   :        +- LogicalRDD [user_id#68, user_name#69], false\n",
            "   +- Filter isnotnull(course_id#0)\n",
            "      +- LogicalRDD [course_id#0, title#1, metadata#2, price#3], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [course_id#71, user_id#70, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69, title#1, metadata#2, price#3]\n",
            "   +- BroadcastHashJoin [course_id#71], [course_id#0], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, is_valid#74, user_name#69]\n",
            "      :  +- BroadcastHashJoin [user_id#70], [user_id#68], LeftOuter, BuildRight, false\n",
            "      :     :- Project [user_id#70, course_id#71, enroll_date_raw#72, enroll_date#73, isnotnull(enroll_date#73) AS is_valid#74]\n",
            "      :     :  +- Project [user_id#70, course_id#71, enroll_date_raw#72, coalesce(gettimestamp(enroll_date_raw#72, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(enroll_date_raw#72, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(enroll_date_raw#72, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS enroll_date#73]\n",
            "      :     :     +- Scan ExistingRDD[user_id#70,course_id#71,enroll_date_raw#72]\n",
            "      :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=88]\n",
            "      :        +- Filter isnotnull(user_id#68)\n",
            "      :           +- Scan ExistingRDD[user_id#68,user_name#69]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=92]\n",
            "         +- Filter isnotnull(course_id#0)\n",
            "            +- Scan ExistingRDD[course_id#0,title#1,metadata#2,price#3]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 5"
      ],
      "metadata": {
        "id": "57nnAJQ8fOuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        " (\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        " (\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        " (\"U003\",\"login|logout\",None,30),\n",
        " (\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]"
      ],
      "metadata": {
        "id": "5IttE2F7fSrx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Schema\n"
      ],
      "metadata": {
        "id": "ofNILCRifVmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, IntegerType\n",
        "\n",
        "activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"actions_raw\", StringType(), True),\n",
        "    StructField(\"metadata_raw\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "iaiMXWvvfevq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Actions\n"
      ],
      "metadata": {
        "id": "-yFFyqKXfh9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_actions(actions):\n",
        "    if actions is None:\n",
        "        return []\n",
        "    if isinstance(actions, list):\n",
        "        return actions\n",
        "    if isinstance(actions, str):\n",
        "        if \",\" in actions:\n",
        "            return actions.split(\",\")\n",
        "        elif \"|\" in actions:\n",
        "            return actions.split(\"|\")\n",
        "        else:\n",
        "            return [actions]\n",
        "    return []"
      ],
      "metadata": {
        "id": "DKHxTna0flaG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Metadata\n"
      ],
      "metadata": {
        "id": "_uyhKLTwfvNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, ast\n",
        "\n",
        "def normalize_metadata(meta):\n",
        "    if meta is None:\n",
        "        return {}\n",
        "    try:\n",
        "        if meta.strip().startswith(\"{\"):\n",
        "            return ast.literal_eval(meta.replace(\"'\", '\"'))\n",
        "        parts = meta.split(\";\")\n",
        "        kv = {}\n",
        "        for p in parts:\n",
        "            if \"=\" in p:\n",
        "                k,v = p.split(\"=\")\n",
        "                kv[k.strip()] = v.strip()\n",
        "        return kv\n",
        "    except:\n",
        "        return {}"
      ],
      "metadata": {
        "id": "BW3wokSGgJQv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fix Raw Data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i6Gae6rmgNxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_activity = [\n",
        "    (uid, normalize_actions(actions), normalize_metadata(meta), duration)\n",
        "    for uid, actions, meta, duration in raw_activity\n",
        "]"
      ],
      "metadata": {
        "id": "xKBq6t9VgUZc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "iDYw4HTWhVsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ActivityDF\").getOrCreate()\n",
        "\n",
        "activity_df = spark.createDataFrame(fixed_activity,\n",
        "    [\"user_id\",\"actions\",\"metadata\",\"duration\"])\n",
        "\n",
        "activity_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MNdy5Zuhd1c",
        "outputId": "d336c229-a6c8-433a-8047-41cb1aa7ace7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |metadata                         |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |{}                               |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explode Actions & Count Frequency"
      ],
      "metadata": {
        "id": "eMeJz3Pohgg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "exploded_df = activity_df.withColumn(\"action\", explode(col(\"actions\")))\n",
        "action_counts = exploded_df.groupBy(\"action\").count()\n",
        "action_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kQCWaL4hhhg",
        "outputId": "4f51f395-89d9-4f5b-e9f4-8d52ff98c6bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|action|count|\n",
            "+------+-----+\n",
            "| watch|    2|\n",
            "|logout|    2|\n",
            "| login|    3|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}